{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charming-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Requires Libraries and Methods\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier, ProximityForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os, csv, re, math, time\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cleared-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.) Load a priori clustered data from the clustering script -> It forms the basis to train the classification model\n",
    "df_hdf = pd.read_hdf(\"data.h5\", \"df_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "angry-connecticut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory already exists, but check existing done and raw directory!\n",
      "Raw data directory already exists\n"
     ]
    }
   ],
   "source": [
    "### 2.) Presets: Required to create a directories for classification and to put data into it\n",
    "###     \"input_data\" - directory contains new input files to classify\n",
    "###     \"done_classification\" - directory contains the preprocssed data -> Actually not required because data will hold in memory, thus no export of preprocessed data are not required\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Check exisiting working directory, if not create one\n",
    "if not os.path.exists(cwd + \"/\" + \"working_dir\"):\n",
    "    os.makedirs(\"working_dir\")\n",
    "    print(\"Working directory successfully created\")\n",
    "    \n",
    "    \"\"\"\n",
    "    if not os.path.exists(cwd + \"/\" + \"working_dir\" + \"/\" + \"done_classification\"):\n",
    "        os.makedirs(cwd + \"/\" + \"working_dir\" + \"/\" + \"done_classification\")\n",
    "        print(\"Done directory successfully created\")\n",
    "    else:\n",
    "        print(\"Done directory already exists\")\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(cwd + \"/\" + \"working_dir\" + \"/\" + \"input_data\"):\n",
    "        os.makedirs(cwd + \"/\" + \"working_dir\" + \"/\" + \"input_data\")\n",
    "    else:\n",
    "        print(\"Raw data directory already exists\")\n",
    "else:\n",
    "    print(\"Working directory already exists, but check existing done and raw directory!\")\n",
    "    \n",
    "    \"\"\"\n",
    "    if not os.path.exists(cwd + \"/\" + \"working_dir\" + \"/\" + \"done_classification\"):\n",
    "        os.makedirs(cwd + \"/\" + \"working_dir\" + \"/\" + \"done_classification\")\n",
    "        print(\"Done directory successfully created\")\n",
    "    else:\n",
    "        print(\"Done directory already exists\")\n",
    "    \"\"\"    \n",
    "    \n",
    "    if not os.path.exists(cwd + \"/\" + \"working_dir\" + \"/\" + \"input_data\"):\n",
    "        os.makedirs(cwd + \"/\" + \"working_dir\" + \"/\" + \"input_data\")\n",
    "        print(\"Raw data directory successfully created\")\n",
    "    else:\n",
    "        print(\"Raw data directory already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "healthy-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.) Prepare (new) input data, thus some data preprocessing are necessary to standardize data\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "# Define path variable\n",
    "# First path variable contains raw data   -  Annotation: Cluster all available data -> move all out data crop folder into raw_data folder; per default only the data from the residuals folder are used\n",
    "WORK_PATH = Path(cwd + \"/\" + \"working_dir\" + \"/\" + \"input_data\")\n",
    "#DONE_PATH = Path(cwd + \"/\" + \"working_dir\" + \"/\" + \"done_classification\")\n",
    "\n",
    "# List all files within working directory and its sub-directories\n",
    "tmp_list = []\n",
    "tmp_part = []        # Total time series (length in total) = 900 = (12*60 ) + (15*12)\n",
    "df_list = []         # 900 Data frames -> each data frames represent one year; length of each Data frames (or time series) are 365\n",
    "\n",
    "filename_only = []\n",
    "\n",
    "for root, dirs, files in sorted(os.walk(WORK_PATH)):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            tmp_list.append(os.path.join(root, file))\n",
    "            \n",
    "# Clear file list, which contains log files like checkpoint in name\n",
    "regex = re.compile(r'-checkpoint.txt')\n",
    "file_list = [i for i in tmp_list if not regex.search(i)]\n",
    "\n",
    "# Process raw data files and convert them to a manageable data format, like csv\n",
    "# At first check if the list of files is empty, if not then continue else skip because no files exists to work with\n",
    "if file_list:\n",
    "    for i, elem in enumerate(file_list):\n",
    "        raw_files = pd.read_csv(file_list[i], sep=\"\\s+\", header=3, usecols=[0,1,6])  #1 Read all files, but only Jahr, Tag and BOF (%nFK)\n",
    "\n",
    "        #2 Alternativ to slicing data (filtering) -> Remove meta information \n",
    "        df_file = raw_files.loc[raw_files[\"Jahr\"] != \"Station:\", \"Jahr\":\"BOF\"]\n",
    "        df_file = raw_files.loc[raw_files[\"Jahr\"] != \"Flexibilisierung:\", \"Jahr\":\"BOF\"]\n",
    "        df_file = raw_files.loc[raw_files[\"Jahr\"] != \"Hauptfrucht:\", \"Jahr\":\"BOF\"]\n",
    "        # Special treatment for Jahr, Tag and BOF\n",
    "        df_file = raw_files.loc[raw_files[\"Jahr\"].str.contains(\"Jahr|mm\") == False, \"Jahr\":\"BOF\"]\n",
    "        \n",
    "        #3 After removing superfluous information reset index to align them \n",
    "        df_file.dropna(inplace=True)\n",
    "        df_file.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #4 Rename columns caption via index\n",
    "        df_file = df_file.rename(columns={df_file.columns[2]: \"BOF (%nFK)\"})\n",
    "\n",
    "        #5 Check the number of days of a year, if the year contains more than 365 days then remove the day 366 -> harmonise data length\n",
    "        indexNames = df_file.loc[df_file[\"Tag\"] == \"366\", \"Jahr\":\"BOF (%nFK)\"].index\n",
    "        df_file = df_file.drop(indexNames)\n",
    "\n",
    "        #6 dtype conversion, because per default all read data are of type object (or string)\n",
    "        # Use int32/float32 instead of int64/float64 by default to save memory\n",
    "        df_file['Jahr'] = df_file['Jahr'].astype('int32')\n",
    "        df_file['Tag'] = df_file['Tag'].astype('int32')\n",
    "        # Special treatment for the col \"BOF (%nFK)\" which objects are comma separation instead dot \n",
    "        # => relevant for conversion\n",
    "        df_file['BOF (%nFK)'] = df_file['BOF (%nFK)'].apply(lambda x: x.replace(',', '.')).astype('float32')\n",
    "\n",
    "        #7 Combine cols \"Jahr\" and \"Tag\" and then convert it to datetime format, ... \n",
    "        # ... whereby Day 1, 2019 can be translated to jan 1st 2019\n",
    "        # a.) Create a new col 'Date'\n",
    "        df_file['Date'] = df_file['Jahr'] * 1000 + df_file['Tag']\n",
    "        \n",
    "        # b.) Convert current date format YYYY-DD to actual date format YYYY-MM-DD\n",
    "        df_file['Date'] = pd.to_datetime(df_file['Date'], format='%Y%j')\n",
    "        \n",
    "        #8 insert column using insert(position, column_name, first_column) function              \n",
    "        df_file.insert(0, 'Date', df_file.pop('Date'))\n",
    "\n",
    "        #9 Omit superfluous columns\n",
    "        df_file = df_file.drop(columns=[\"Jahr\", \"Tag\"])\n",
    "        \n",
    "        #10 Set Date as (time) index\n",
    "        df_file.set_index('Date', inplace=True)\n",
    "        df_file.sort_index(inplace=True)\n",
    "        \n",
    "        #11 Partition dataframe by years -> Required to treat cluster daily based data for a year as data point (thus clustering whole year)\n",
    "        for j in df_file.groupby(pd.DatetimeIndex(df_file.index).year):\n",
    "            tmp_part.append(j)\n",
    "        \n",
    "        #12 Filenames without extension .txt\n",
    "        #base = os.path.basename(file_list[i])\n",
    "        #tmp = os.path.splitext(base)\n",
    "        #filename_only.append(tmp[0])\n",
    "        \n",
    "        #12 Export processed data -> Not requried, but can be used to check, if the the data are processed correctly, but for now its will be commented\n",
    "        #export_csv = df_file.to_csv(str(DONE_PATH) + \"/\" + filename_only[i] + \".csv\", sep=\";\", index=True, header=True, encoding=\"utf-8\")\n",
    "else:\n",
    "    print(\"List is empty, hence no files in directory to process\")\n",
    "    \n",
    "\n",
    "#12  Get dataframe from the tuple -> extract from partition only the data frames, e.g. [(1961, dataframe)]\n",
    "for k, df in enumerate(tmp_part):\n",
    "    df_list.append(df[1])\n",
    "    \n",
    "\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "discrete-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.) Convert each time series from pd.dataframe from to pd.series\n",
    "df_list_series = []\n",
    "\n",
    "for i, series in enumerate(df_list):\n",
    "    df_tmp = df_list[i].squeeze()\n",
    "    df_list_series.append(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "modified-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.) Create label 'year'\n",
    "year_tmp = []\n",
    "\n",
    "for i, elem in enumerate(tmp_part):\n",
    "    year_tmp.append(elem[0])\n",
    "\n",
    "df_data = pd.DataFrame({\"Jahr\": year_tmp, \"Series\": df_list_series})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.) Presets for classification -> Applies only for the new input data!\n",
    "\n",
    "# Quick normalize time series data -> [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_list_norm = []\n",
    "\n",
    "for n, foo in enumerate(df_list):\n",
    "    tmp = scaler.fit_transform(df_list[n])\n",
    "    \n",
    "    df_list_norm.append(tmp)\n",
    "\n",
    "# Reshape required for clustering \n",
    "for m, fool in enumerate(df_list_norm):\n",
    "    df_list_norm[m] = df_list_norm[m].reshape(len(df_list_norm[m]))\n",
    "    \n",
    "    \n",
    "# del n, foo, m, fool, tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "breathing-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.) Required for classsification based on the clustered data! (not for the new input data) \n",
    "\n",
    "X_data = df_hdf[\"Series\"]\n",
    "X_data = X_data.to_frame()                       # Take from the hdf5 file the first column, which contains the BOF (%nFK) and convert from series to data frame type\n",
    "\n",
    "y_data = df_hdf[\"Labels\"]\n",
    "y_data = y_data.to_numpy()                       # Similar for the column with assigned labels, but to numpy array type \n",
    "\n",
    "# Split data in train and test samples -> Ratio 70:30 (rule of thumb -> adjustable depending on preference via parameter \"test_size\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "permanent-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.) Required for the new input data, which are classified \n",
    "\n",
    "X_classify = df_data[\"Series\"]\n",
    "X_classify = X_classify.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8.) Apply Classifiers\n",
    "\n",
    "# A.) KNN - Tuning parameter: n_neighbors=n  -> tends to overfitting and sensitve to outliers, not good for large data sets -> not recommended!\n",
    "knn_classifier = KNeighborsTimeSeriesClassifier(n_neighbors=1 ,distance=\"euclidean\").fit(X_train, y_train)\n",
    "\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Measure accuracy\n",
    "accuracy_score(y_test, knn_pred)\n",
    "confusion_matrix(y_test, knn_pred)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "\n",
    "# The following classifiers are useing the concept of ensemble learning\n",
    "# That means: \n",
    "#    -> Instead one strong learner (decision tree) there are n-weak learners, which are trained on subset of the given data sets\n",
    "#    -> n-weak learners or decision tress are trained and buil up to a meta model with result\n",
    "#    -> Pro: Prevent under-/ overfitting and is not subject under random influence, thus  yield better classification result\n",
    "#    -> Cons: requires higher computational ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "foreign-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.) TimeSeriesForestClassifier - Tuning parameter: n_estimators=n (Number of estimators to build for the ensemble)\n",
    "\n",
    "#B.1  Define and call classification model and train via fit()-function -> fit() takes the first argument the train data and as second the target variable, which contains the labels that belongs to the train data\n",
    "forest_classifier = TimeSeriesForestClassifier(n_estimators=5).fit(X_train, y_train)\n",
    "\n",
    "#B.2 Once the training is complete, we can now see how well the model predicts the \"new\" data, which in this case is the previously omitted or split data.  \n",
    "#forest_pred = forest_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score\n",
    "forest_score = forest_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tender-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.) Classify new input data\n",
    "forest_pred_input_data = forest_classifier.predict(X_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "documented-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.) Assign classification result to the input data\n",
    "df_data.insert(loc=2, column=\"class\", value=forest_pred_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "informal-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/jupyterlab/opt/anaconda3/envs/jupyterlab/lib/python3.8/site-packages/pandas/core/generic.py:2606: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->Index(['Series'], dtype='object')]\n",
      "\n",
      "  pytables.to_hdf(\n"
     ]
    }
   ],
   "source": [
    "### 11.) Export result in hdf5 and csv (where the latter is more for checking -> optional)\n",
    "# Export via hdf will yield a warning, but can be ignored\n",
    "export_class_hdf = df_data.to_hdf(\"data_class.hdf5\", key=\"df_data\", mode=\"w\" ) \n",
    "export_class_csv = df_data.to_csv(\"data_class.csv\", sep=\";\", index=True, header=True, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.) ProximityForest (distanced-based) - Calculation takes longer than the previous two\n",
    "# Tuning parameter: n_estimators=n (The number of trees in the forest), max_depth=4 (maximum depth of the tree) --> Playground \n",
    "prox_classifier = ProximityForest(random_state=22, n_estimators=5, max_depth=4, \n",
    "                                  distance_measure=\"euclidean\").fit(X_data, y_data)\n",
    "\n",
    "# Accuracy Score\n",
    "prox_score = prox_classifier.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
